<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <!-- Begin Jekyll SEO tag v2.7.1 -->
  <title>FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="title">
  <meta property="og:locale" content="en_US">
  <meta name="twitter:card" content="summary">
  <!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
  <style>
        .method {
            display: inline-block;
            font-weight: bold;
        }

        .explanation {
            display: inline-block;
        }
    </style>
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->


  </section>

  <section class="main-content">
    <h1 id="">
      <center>FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech</center>
    </h1>

    <h3 id="">
      <center>A work submitted to ACMMM 2025</center>
    </h3>



    <br><br>
    <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
    <p>Current speech generation research can be categorized into two primary classes: non-autoregressive (NAR) and 
      autoregressive (AR). The fundamental distinction between these approaches lies in the duration prediction strategy
      employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and 
      independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to 
      predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves 
      prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of 
      stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. 
      The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration 
      predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. 
      Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic 
      model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference 
      audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic 
      variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre.
      Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, 
      when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with 
      about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.</p>
    <br><br>

        <center><img src='raw/fig/overview_new.png' width="70%"></center>
        <br><br>
        <center><span><b>Figure 1. Overview of FlexSpeech. (a). The architecture of FlexSpeech acoustic model. 
          The phoneme embeddings, repeated speaker embeddings, and random noise are concatenated along the channel 
          dimension to form the input to the model, which then predicts a mel-spectrogram of the same length. 
          (b). Duration model and preference alignment. The decoder predicts current phoneme duration in an 
          autoregressive manner, based on the hidden state and previous phoneme durations.</b></span></center>

    <br><br>


    <h2>2. Comparison with Baselines <a name="Comparison"></a></h2>

    <table>
      <tbody id="tbody_speech">
      </tbody>
    </table>

    <h2>3. Zero-shot Voice Cloning <a name="zero-shot"></a></h2>
    <table>
      <tbody id="tbody_new_comparison">
      </tbody>
    </table>

    <h2>4. Rapid Style Transfer <a name="style-transfer"></a></h2>
    <table>
      <tbody id="rapid_style_transfer">
      </tbody>
    </table>


    <h2>5. Controllability <a name="Controllability"></a></h2>
    <table>
      <tbody id="tbody_custom">
        <!-- 表头 -->
        <tr>
          <td style="text-align: center; width: 33%;"><strong> </strong></td>
          <td style="text-align: center; width: 33%;"><strong> </strong></td>
          <td style="text-align: center; width: 33%;"><strong> </strong></td>
        </tr>
        <!-- 第一行：audio方式 -->
        <tr>
          <td style="text-align: center"><audio style="width: 100%;" controls="" src="./raw/samples/model1/1.wav"></audio></td>
          <td style="text-align: center"><audio style="width: 100%;" controls="" src="./raw/samples/model2/1.wav"></audio></td>
          <td style="text-align: center"><audio style="width: 100%;" controls="" src="./raw/samples/model3/1.wav"></audio></td>
        </tr>
        <!-- 第二行：图片 -->
        <tr>
          <td style="text-align: center"><img src="./raw/images/image1.png" width="100%"></td>
          <td style="text-align: center"><img src="./raw/images/image2.png" width="100%"></td>
          <td style="text-align: center"><img src="./raw/images/image3.png" width="100%"></td>
        </tr>
      </tbody>
    </table>

</html>





<script type="text/javascript">
  function short_form() {
    let scenes = [
      [0], [1], [2], [3], [4], [5], [6], [7]
    ];
    let models = ["prompt speech", "FlexSpeech", "F5-TTS", "MaskGCT", "CosyVoice", "FireRedTTS"];
    let texts = [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
    ];
    let short_data = "";

    // 生成表头
    short_data += '<tr>';
    short_data += '<td style="text-align: center; width: 14%;"><strong>Text</strong></td>'; // 新增文本列
    for (const id in models) {
      let model = models[id];
      short_data += '<td style="text-align: center; width: 14%;" rowspan="1"><strong>' + model + '</strong></td>';
    }
    short_data += '</tr>';

    // 生成表格内容
    for (let x in scenes) {
      let scene = scenes[x];
      let file = scene[0];
      let scene_data = "";

      scene_data += '<tr>';
      scene_data += '<td style="text-align: center"> ' + texts[x] + '</td>'; // 文本内容
      for (let z in models) {
        let model = models[z];
        scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './raw/samples/' + model + '/' + file + '.wav' + '"></audio></td>';
      }
      scene_data += '</tr>';
      short_data += scene_data;
    }

    return short_data;
  }

  function new_comparison_form() {
    let new_scenes = [
      [11], [12], [13], [14], [15], [16]
    ];
    let new_models = ["prompt speech", "FlexSpeech Generated"];
    let new_texts = [
      "",
      "",
      "",
      "",
      "",
      "",
    ];
    let new_data = "";

    // 生成表头
    new_data += '<tr>';
    new_data += '<td style="text-align: center; width: 14%;"><strong>Text</strong></td>'; // 新增文本列
    for (const id in new_models) {
      let model = new_models[id];
      new_data += '<td style="text-align: center; width: 14%;" rowspan="1"><strong>' + model + '</strong></td>';
    }
    new_data += '</tr>';

    // 生成表格内容
    for (let x in new_scenes) {
      let scene = new_scenes[x];
      let file = scene[0];
      let scene_data = "";

      scene_data += '<tr>';
      scene_data += '<td style="text-align: center">' + new_texts[x] + '</td>'; // 文本内容
      for (let z in new_models) {
        let model = new_models[z];
        scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './raw/samples/' + model + '/' + file + '.wav' + '"></audio></td>';
      }
      scene_data += '</tr>';
      new_data += scene_data;
    }

    return new_data;
  }


  function style_transfer_form() {
    let new_scenes = [
      [21], [22], [23], [24], [25], [26], [27], [28]
    ];
    let new_models = ["style prompt speech", "timbre reference speaker", "FlexSpeech Generated" ];
    let new_texts = [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
    ];
    let new_data = "";

    // 生成表头
    new_data += '<tr>';
    new_data += '<td style="text-align: center; width: 14%;"><strong>Text</strong></td>'; // 新增文本列
    for (const id in new_models) {
      let model = new_models[id];
      new_data += '<td style="text-align: center; width: 14%;" rowspan="1"><strong>' + model + '</strong></td>';
    }
    new_data += '</tr>';

    // 生成表格内容
    for (let x in new_scenes) {
      let scene = new_scenes[x];
      let file = scene[0];
      let scene_data = "";

      scene_data += '<tr>';
      scene_data += '<td style="text-align: center">' + new_texts[x] + '</td>'; // 文本内容
      for (let z in new_models) {
        let model = new_models[z];
        scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './raw/samples/' + model + '/' + file + '.wav' + '"></audio></td>';
      }
      scene_data += '</tr>';
      new_data += scene_data;
    }

    return new_data;
  }

  window.onload = function () {
    document.getElementById('tbody_speech').innerHTML = short_form();
    document.getElementById('tbody_new_comparison').innerHTML = new_comparison_form();
    document.getElementById('rapid_style_transfer').innerHTML = style_transfer_form();
  }
</script>
